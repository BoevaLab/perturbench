{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator API Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import anndata as ad\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import perturbench.analysis.benchmarks as benchmarks\n",
    "from perturbench.data.datasplitter import PerturbationDataSplitter\n",
    "from perturbench.analysis.benchmarks.evaluator import Evaluator\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we'll be demonstrating the usage of the Evaluator API using the srivatsan20-transfer task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cache_dir = '../neurips2024/perturbench_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 183856 × 9198\n",
       "    obs: 'ncounts', 'well', 'plate', 'cell_line', 'replicate', 'time', 'dose_value', 'pathway_level_1', 'pathway_level_2', 'perturbation', 'target', 'pathway', 'dose_unit', 'celltype', 'disease', 'cancer', 'tissue_type', 'organism', 'perturbation_type', 'ngenes', 'percent_mito', 'percent_ribo', 'nperts', 'chembl-ID', 'dataset', 'cell_type', 'treatment', 'condition', 'dose', 'cov_merged', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes'\n",
       "    var: 'ensembl_id', 'ncounts', 'ncells', 'gene_symbol', 'n_cells', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'highly_variable_nbatches'\n",
       "    uns: 'hvg', 'log1p', 'rank_genes_groups_cov'\n",
       "    layers: 'counts'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata = sc.read_h5ad(f'{data_cache_dir}/srivatsan20_processed.h5ad')\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a datasplit using our data splitting class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_splitter = PerturbationDataSplitter(\n",
    "    adata.obs.copy(),\n",
    "    perturbation_key='condition',\n",
    "    covariate_keys=['cell_type'],\n",
    "    perturbation_control_value='control',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a split. Setting a seed will ensure you get the same split every time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = transfer_splitter.split_covariates(\n",
    "    seed=0, \n",
    "    print_split=True, ## Print a summary of the split if True\n",
    "    max_heldout_covariates=2, ## Maximum number of held out covariates (in this case cell types)\n",
    "    max_heldout_fraction_per_covariate=0.3, ## Maximum fraction of perturbations held out per covariate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29635, 9198)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata_test = adata[split == 'test']\n",
    "adata_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll simulate predictions by randomly subsampling and shuffling the data and treating those samples/shuffles as different \"model predictions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12261/724230800.py:1: ImplicitModificationWarning: Trying to modify attribute `.obs` of view, initializing view as actual.\n",
      "  adata_test.obs['condition_cell_type'] = adata.obs['condition'].astype(str) + '_' + adata.obs['cell_type'].astype(str)\n"
     ]
    }
   ],
   "source": [
    "adata_test.obs['condition_cell_type'] = adata.obs['condition'].astype(str) + '_' + adata.obs['cell_type'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_cells, _ = train_test_split(\n",
    "    adata_test.obs_names,\n",
    "    test_size=0.25, \n",
    "    stratify=adata_test.obs['condition_cell_type'],\n",
    "    random_state=54\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22226, 9198)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_adata = adata[sampled_cells, :]\n",
    "sampled_adata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create the shuffled predictions to serve as a negative control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22226, 9198)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(54)\n",
    "\n",
    "random_adata_list = []\n",
    "for cell_type in sampled_adata.obs.cell_type.unique():\n",
    "    random_adata_cl = sampled_adata[sampled_adata.obs.cell_type == cell_type, :].copy()\n",
    "    random_adata_cl.obs['condition'] = random.sample(\n",
    "        list(random_adata_cl.obs['condition'].astype(str)), \n",
    "        k=random_adata_cl.n_obs,\n",
    "    )\n",
    "    random_adata_list.append(random_adata_cl)\n",
    "\n",
    "random_adata = ad.concat(random_adata_list)\n",
    "random_adata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List all tasks in the Evaluator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['srivatsan20-transfer', 'norman19-combo', 'mcfaline23-transfer']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluator.list_tasks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an evaluator object with the srivatsan20-transfer task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "srivatsan20_eval = benchmarks.evaluator.Evaluator(\n",
    "    task='srivatsan20-transfer',\n",
    "    local_data_cache=data_cache_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to our evaluator class is a dictionary of model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_predictions = {\n",
    "    'sampled': sampled_adata,\n",
    "    'random': random_adata,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then evaluate our simulated model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = srivatsan20_eval.evaluate(\n",
    "    model_predictions=simulated_predictions,\n",
    "    return_metrics_dataframe=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then look at the summary metrics returned by the evaluation. This is an average of the metric computed on a per-perturbation basis. As we can see, the sampled data is very close to the full observed data and the random data has no information at all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>model</th>\n",
       "      <th>random</th>\n",
       "      <th>sampled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse_average</th>\n",
       "      <td>0.028210</td>\n",
       "      <td>0.005487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse_rank_average</th>\n",
       "      <td>0.489300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosine_logfc</th>\n",
       "      <td>0.000577</td>\n",
       "      <td>0.890200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cosine_rank_logfc</th>\n",
       "      <td>0.509800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model                random   sampled\n",
       "rmse_average       0.028210  0.005487\n",
       "rmse_rank_average  0.489300  0.000000\n",
       "cosine_logfc       0.000577  0.890200\n",
       "cosine_rank_logfc  0.509800  0.000000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perturbench-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
