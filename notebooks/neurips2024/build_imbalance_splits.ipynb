{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8c496b6-b613-441d-aa7d-50fbc1262fc1",
   "metadata": {},
   "source": [
    "# Investigate data imbalance\n",
    "Start with Srivatsan balanced data (189 perturbationsn seen in three cell types). Create imbalanced splits through downsampling.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7346c620-d58c-4a15-be2f-227bbab0648f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from perturbench.data.datasplitter import PerturbationDataSplitter\n",
    "import warnings\n",
    "import os\n",
    "# from scipy.stats import entropy\n",
    "\n",
    "# Suppress FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20e0fcf-7fdc-47b6-bc04-18246d3dca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_train_cell_types(\n",
    "    splitter,\n",
    "    split_key,\n",
    "):\n",
    "    \"\"\"Returns the number of training cell types per perturbation\"\"\"\n",
    "    num_train_cell_types = []\n",
    "    for pert in splitter.obs_dataframe.condition.unique():\n",
    "        pert_df = splitter.obs_dataframe[splitter.obs_dataframe.condition == pert]\n",
    "        pert_df = pert_df.loc[:,['cell_type', split_key]].drop_duplicates()\n",
    "        num_train_cell_types.append(pert_df.loc[pert_df[split_key] == 'train', 'cell_type'].nunique())\n",
    "\n",
    "    num_train_cell_types = pd.Series(num_train_cell_types, index=splitter.obs_dataframe.condition.unique())\n",
    "    return num_train_cell_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e55a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cache_dir = './perturbench_data' ## Change this to your local data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0997b76a-c038-4ef4-bcb4-51b1dbd55833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 183856 × 9198\n",
       "    obs: 'ncounts', 'well', 'plate', 'cell_line', 'replicate', 'time', 'dose_value', 'pathway_level_1', 'pathway_level_2', 'perturbation', 'target', 'pathway', 'dose_unit', 'celltype', 'disease', 'cancer', 'tissue_type', 'organism', 'perturbation_type', 'ngenes', 'percent_mito', 'percent_ribo', 'nperts', 'chembl-ID', 'dataset', 'cell_type', 'treatment', 'condition', 'dose', 'cov_merged', 'n_genes_by_counts', 'log1p_n_genes_by_counts', 'total_counts', 'log1p_total_counts', 'pct_counts_in_top_50_genes', 'pct_counts_in_top_100_genes', 'pct_counts_in_top_200_genes', 'pct_counts_in_top_500_genes'\n",
       "    var: 'ensembl_id', 'ncounts', 'ncells', 'gene_symbol', 'n_cells', 'n_cells_by_counts', 'mean_counts', 'log1p_mean_counts', 'pct_dropout_by_counts', 'total_counts', 'log1p_total_counts', 'highly_variable', 'highly_variable_rank', 'means', 'variances', 'variances_norm', 'highly_variable_nbatches'\n",
       "    uns: 'hvg', 'log1p', 'rank_genes_groups_cov'\n",
       "    layers: 'counts'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_transfer_adata = sc.read_h5ad(f'{data_cache_dir}/srivatsan20_processed.h5ad')\n",
    "balanced_transfer_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "456fe74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell_type\n",
      "a549    189\n",
      "k562    189\n",
      "mcf7    189\n",
      "Name: perturbation, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "unique_perturbations = balanced_transfer_adata.obs.groupby('cell_type')['perturbation'].nunique()\n",
    "print(unique_perturbations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f2188d2-6473-4229-84d0-0eae88ba0fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<perturbench.data.datasplitter.PerturbationDataSplitter at 0x7f0ad2bde550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_transfer_splitter = PerturbationDataSplitter(\n",
    "    balanced_transfer_adata.obs.copy(),\n",
    "    perturbation_key='condition',\n",
    "    covariate_keys=['cell_type'],\n",
    "    perturbation_control_value='control',\n",
    ")\n",
    "balanced_transfer_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09722c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_entropy(counts): \n",
    "    counts = counts + 1e-8 # to avoid -inf\n",
    "    N = np.sum(counts) \n",
    "    probabilities = counts / N \n",
    "    log_probabilities = np.log(probabilities) \n",
    "    entropy = -np.sum(probabilities * log_probabilities) \n",
    "    normalized_entropy = entropy / np.log(len(counts)) \n",
    "    return normalized_entropy \n",
    "\n",
    "def find_array_with_entropy(target_entropy, max_value=189, tolerance=0.01, max_iter=10000, seed=42):  \n",
    "    np.random.seed(seed)\n",
    "    for _ in range(max_iter):  \n",
    "        # Generate two random values less than or equal to max_value  \n",
    "        # np.random.seed(0) \n",
    "        x1, x2 = np.random.randint(30, max_value+1, 2)  \n",
    "        counts = np.array([189, x1, x2])  \n",
    "        current_entropy = balanced_entropy(counts)  \n",
    "        # Check if the current entropy is within the tolerance of the target entropy  \n",
    "        if np.abs(current_entropy - target_entropy) < tolerance:  \n",
    "            return counts  \n",
    "    return None  # Return None if no solution is found within max_iter iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aa5d405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6546548083217102"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Minimum possible entropy requiring at least 30 perts per cell type\n",
    "x = np.array([189, 30, 30])\n",
    "x.shape\n",
    "balanced_entropy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18be8d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found array: [189  34  34]\n",
      "0.692879243111273\n"
     ]
    }
   ],
   "source": [
    "# Example usage and deciding reasonable values for target entropy (too low and no such distribution can be found)  \n",
    "# Set a target entropy value  \n",
    "target_entropy = 0.7  # Example value  \n",
    "target_distribution = find_array_with_entropy(target_entropy)  \n",
    "if target_distribution is not None:  \n",
    "    print('Found array:', target_distribution)  \n",
    "    print(balanced_entropy(target_distribution))\n",
    "else:  \n",
    "    print('No array found that meets the criteria within the given iterations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "577da164",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_adata(balanced_transfer_adata, desired_unique_perturbations, seed=42): \n",
    "    np.random.seed(seed)   # Group the perturbations by cell type and count the number of unique perturbations \n",
    "    \n",
    "    unique_perturbations = balanced_transfer_adata.obs.groupby('cell_type')['perturbation'].nunique() \n",
    " \n",
    "    # Create a mask to filter the data \n",
    "    mask = np.zeros(len(balanced_transfer_adata), dtype=bool) \n",
    " \n",
    "    # Iterate over each cell type and select the desired number of unique perturbations \n",
    "    for cell_type, num_unique_perturbations in zip(unique_perturbations.index, desired_unique_perturbations): \n",
    "        cell_type_mask = (balanced_transfer_adata.obs['cell_type'] == cell_type) \n",
    "        perturbations = balanced_transfer_adata.obs.loc[cell_type_mask, 'perturbation'].unique()\n",
    "        \n",
    "        np.random.shuffle(perturbations) \n",
    "        \n",
    "        selected_perturbations = list(perturbations[:num_unique_perturbations]) \n",
    "        if 'control' not in selected_perturbations:\n",
    "            selected_perturbations.append('control')\n",
    "        mask |= (cell_type_mask & balanced_transfer_adata.obs['perturbation'].isin(selected_perturbations)) \n",
    " \n",
    "    # Apply the mask to create the downsampled AnnData object \n",
    "    adata_downsampled = balanced_transfer_adata[mask]#.copy() \n",
    "    # adata_downsampled.obs.reset_index(drop=True, inplace=True)  # Reset index to avoid any indexing issues\n",
    "    adata_downsampled = adata_downsampled.copy()  # Ensure that the data matrix is realigned\n",
    "\n",
    "    return adata_downsampled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf19582a",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dir = f'{data_cache_dir }/downsampled_imbalance/'\n",
    "\n",
    "if not os.path.exists(split_dir):\n",
    "    os.makedirs(split_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "248b953d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13221/1339905266.py:14: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(perturbations)\n",
      "/tmp/ipykernel_13221/1339905266.py:14: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(perturbations)\n",
      "/tmp/ipykernel_13221/1339905266.py:14: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(perturbations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split summary: \n",
      "                  train  val  test\n",
      "('none', 'a549')    153   19    19\n",
      "('none', 'k562')     23    7     6\n",
      "('none', 'mcf7')     24    6     7\n",
      "0.7 [189  34  34]\n",
      "train    64245\n",
      "test     14265\n",
      "val      13319\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13221/1339905266.py:14: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(perturbations)\n",
      "/tmp/ipykernel_13221/1339905266.py:14: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(perturbations)\n",
      "/tmp/ipykernel_13221/1339905266.py:14: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(perturbations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split summary: \n",
      "                  train  val  test\n",
      "('none', 'a549')    132   30    29\n",
      "('none', 'k562')     57   13    14\n",
      "('none', 'mcf7')     22    6     6\n",
      "0.8 [189  82  31]\n",
      "train    65458\n",
      "test     17689\n",
      "val      16857\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13221/1339905266.py:14: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(perturbations)\n",
      "/tmp/ipykernel_13221/1339905266.py:14: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(perturbations)\n",
      "/tmp/ipykernel_13221/1339905266.py:14: UserWarning: you are shuffling a 'Categorical' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(perturbations)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split summary: \n",
      "                  train  val  test\n",
      "('none', 'a549')    132   29    30\n",
      "('none', 'k562')     35    9     9\n",
      "('none', 'mcf7')     82   19    19\n",
      "0.9 [189  51 118]\n",
      "train    84739\n",
      "val      23339\n",
      "test     20364\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "entropy_list =  [0.7,0.8,0.9]\n",
    "for target_entropy in entropy_list:\n",
    "    target_distribution = find_array_with_entropy(target_entropy) \n",
    "    adata_downsampled = downsample_adata(balanced_transfer_adata, target_distribution)\n",
    "    \n",
    "    splitter = PerturbationDataSplitter(\n",
    "        adata_downsampled.obs.copy(),\n",
    "        perturbation_key='condition',\n",
    "        covariate_keys=['cell_type', 'treatment'],\n",
    "        perturbation_control_value='control',\n",
    "    )\n",
    "\n",
    "    split = splitter.split_covariates(\n",
    "    seed=57,\n",
    "    print_split=True, ## Print a summary of the split if True\n",
    "    max_heldout_covariates=1, ## Maximum number of held out covariates (in this case cell types)\n",
    "    max_heldout_fraction_per_covariate=0.3, ## Maximum fraction of perturbations held out per covariate\n",
    ")\n",
    "    # Cells to be excluded in the downsample are given 'None' as their label so they don't go in train/validation/test\n",
    "    split_padded = pd.Series(None, index=balanced_transfer_adata.obs.index)\n",
    "    split_padded.loc[split.index] = split\n",
    "    split_padded.to_csv(f'{split_dir}/srivatsan_downsampled_entropy_' + str(target_entropy) + '_splits.csv', header=False)\n",
    "    print(target_entropy,target_distribution)\n",
    "    print(split_padded.value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278c54c",
   "metadata": {},
   "source": [
    "### Fully balanced split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d14c07e8-ea9d-46cf-8390-6b37c63bcfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split summary: \n",
      "                  train  val  test\n",
      "('none', 'a549')    132   30    29\n",
      "('none', 'k562')    132   29    30\n",
      "('none', 'mcf7')    132   29    30\n"
     ]
    }
   ],
   "source": [
    "balanced_transfer_splitter = PerturbationDataSplitter(\n",
    "    balanced_transfer_adata.obs.copy(),\n",
    "    perturbation_key='condition',\n",
    "    covariate_keys=['cell_type','treatment'],\n",
    "    perturbation_control_value='control',\n",
    ")\n",
    "\n",
    "\n",
    "balanced_transfer_split = balanced_transfer_splitter.split_covariates(\n",
    "            seed=57, \n",
    "            print_split=True, ## Print a summary of the split if True\n",
    "            max_heldout_covariates=1, ## Maximum number of held out covariates (in this case cell types)\n",
    "            max_heldout_fraction_per_covariate=0.3, ## Maximum fraction of perturbations held out per covariate\n",
    "        )\n",
    "\n",
    "balanced_transfer_split.to_csv(f'{split_dir}/srivatsan_downsampled_entropy_' + str(1) + '_splits.csv', header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "perturbench-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
