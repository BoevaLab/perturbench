# @package _global_

#for HPO runs.
defaults:
  - override /hydra/sweeper: optuna
  - override /hydra/launcher: ray
  - override /hydra/sweeper/sampler: tpe

## Specify multiple metrics and how they should be added together
metrics_to_optimize:
  rmse_average: 1.0
  rmse_rank_average: 0.1

hydra:
  mode: "MULTIRUN" # set hydra to multirun by default if this config is attached
  sweeper:
    direction: minimize
    study_name: ${model._target_}_${oc.env:USER}_${now:%Y-%m-%d}/${now:%H-%M-%S} # if there is an existing study with the same name, it will be resumed
    storage: null # optuna database to store HPO results. 
    n_trials: 4  # number of total trials
    n_jobs: 2 # number of parallel jobs
    max_failure_rate: 1 # overall HPO job will not fail if less than this ratio of trials fail
    params:
      model.n_layers_encoder: range(1, 7, step=2)
      model.n_layers_pert_emb: range(1, 5, step=1)
      model.adv_classifier_n_layers: range(1, 5, step=1)
      model.hidden_dim: range(256, 5376, step=1024)
      model.adv_classifier_hidden_dim: tag(log, int(interval(128, 1024))) # log scale
      model.adv_steps: choice(2, 3, 5, 7, 10, 20, 30)
      model.n_latent: choice(64, 128, 192, 256, 512)
      model.lr: tag(log, interval(5e-6, 1e-3)) 
      model.wd: tag(log, interval(1e-8, 1e-3))
      model.dropout: range(0.0, 0.8, step=0.1)
      model.kl_weight: tag(log, interval(0.1, 20))
      model.adv_weight: tag(log, interval(0.1, 20))
      model.penalty_weight: tag(log, interval(0.1, 20))
  launcher:
    ray:
      init:
        ## for local runs
        num_gpus: 2 # number of total gpus to use
      remote:
        num_gpus: 1 # number of gpus per trial
        max_calls: 1